{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7469c28-2329-4482-a639-ef9bf5617278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  8217k      0  0:00:09  0:00:09 --:--:-- 16.1M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1557af8-7cf6-4be0-8dc1-d0ff0ee0821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b45b3f-6dad-45ad-9727-2610db467fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30874a8-b7a7-44ff-82a5-a9ed9b5d5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037a15e0-2dc3-4ad3-bf0c-cb78e86f80b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1981020-0612-420c-a032-7cab1dd26684",
   "metadata": {},
   "source": [
    "## 20k files for training, 5k files for validation and 25k files for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b136978f-b7d6-4858-a01c-b0352046ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\\\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "            val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aae063-77b7-41ca-8253-665efcb2a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b226e54-4f64-4c01-92f3-dacbb48cb7d8",
   "metadata": {},
   "source": [
    "# Data Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707bf98c-13e2-49f6-984f-2fd26fd6ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"Man With the Gun is pretty much forgotten now, but caused a minor storm of media interest back in 1955 when Robert Mitchum turned down both Jett Rink in Giant (which had actually been written for him and which was subsequently substantially reworked) and Charles Laughton's intended version of The Naked and the Dead to make it instead. Despite some obvious production problems and some harsh lighting that occasionally renders both Mitch and Jan Sterling in unflattering tones, it's a terrific dark western that more than stands comparison with his earlier Blood on the Moon as his 'town tamer' sets to work on a town that never had the chance to grow up before getting run down by the local badmen before turning out to \\xc2\\x96 possibly \\xc2\\x96 be almost as bad as the men he dispatches. Certainly his way of dealing with news of a death in the family \\xc2\\x96 burning a saloon to the ground and goading its manager into trying to kill him \\xc2\\x96 doesn't inspire much confidence in his stability. As well as a good script and a surprisingly good supporting turn from the usually irritating but here well cast Henry Hull, it also boasts a strikingly good early Alex North score, which even includes an early workout for one of his tormented emotional cues that would later turn up in Spartacus.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64003677-6ab0-4092-882c-a652d354dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one of the worst movies i', 'worst movie i have ever seen', 'the worst movie i have ever', 'this is one of the worst', 'of the worst movies i have', 'the worst movie i ve ever', 'worst movie i ve ever seen', 'the worst movies i have ever', 'worst movies i have ever seen', 'one of the worst films i', 'to be one of the worst', 'of the worst movies i ve', 'is one of the worst movies', 'one of the worst movies ever', 'worst movies i ve ever seen', 'don t waste your time or', 'don t waste your time on', 'the worst movies i ve ever', 'the worst film i have ever', 'don t waste your time with', 'worst film i have ever seen', 'of the worst films i have', 't waste your time on this', 'worst film i ve ever seen', 't waste your time or money', 'of the worst movies ever made', 't waste your time with this', 'don t waste your time watching', 'worst films i have ever seen', 'the worst film i ve ever']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "directory = \"aclImdb/train/neg\"\n",
    "\n",
    "# Words to filter n-grams\n",
    "\n",
    "\n",
    "\n",
    "phrase_freq = Counter()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'\\bbr\\b', '', text)\n",
    "    cleaned_word_list = re.findall(r'\\b\\w+\\b', cleaned_text.lower())\n",
    "    return cleaned_word_list\n",
    "\n",
    "\n",
    "def contains_filter_word(phrase):\n",
    "    filter_phrases = [\"worst\", \"waste\", \"time wasting\", \"racist\",\"crap\", \"disappointed\", \"so bad\", \"don't watch\"]\n",
    "    for filter_phrase in filter_phrases:\n",
    "        if filter_phrase in ' '.join(phrase):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        words = clean_text(text)\n",
    "        # Generate n-grams and update phrase frequencies\n",
    "        n_grams = ngrams(words, 6)\n",
    "        filtered_ngrams = filter(contains_filter_word, n_grams)\n",
    "        phrase_freq.update(filtered_ngrams)\n",
    "\n",
    "# Print the most common phrases\n",
    "#print(\"Most common filtered phrases:\")\n",
    "#for phrase, freq in phrase_freq.most_common(50):\n",
    "#    print(f\"{' '.join(phrase)}: {freq}\")\n",
    "\n",
    "top_phrases = [' '.join(phrase) for phrase, _ in phrase_freq.most_common(30)]\n",
    "print(top_phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a828cd-7484-4510-a266-665e8c50e243",
   "metadata": {},
   "source": [
    "# Bag-of-words approach: Bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35a7bd16-db36-4feb-92a6-9cce925764da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "851cb1c1-4a31-4234-b7c5-0446a19c918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "ngrams=2,\n",
    "max_tokens=20000,\n",
    "output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e83918d6-6977-4b32-893b-42d3bf0aa554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87c3ca37-4fa9-47dc-8a40-56a311c74165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 16s 24ms/step - loss: 0.3679 - accuracy: 0.8525 - val_loss: 0.2818 - val_accuracy: 0.8878\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.2317 - accuracy: 0.9191 - val_loss: 0.2900 - val_accuracy: 0.8896\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.2004 - accuracy: 0.9333 - val_loss: 0.3095 - val_accuracy: 0.8904\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.1813 - accuracy: 0.9430 - val_loss: 0.3264 - val_accuracy: 0.8946\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.1677 - accuracy: 0.9493 - val_loss: 0.3425 - val_accuracy: 0.8886\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.1648 - accuracy: 0.9518 - val_loss: 0.3614 - val_accuracy: 0.8904\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.1566 - accuracy: 0.9549 - val_loss: 0.3919 - val_accuracy: 0.8874\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.1634 - accuracy: 0.9563 - val_loss: 0.3925 - val_accuracy: 0.8890\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.1464 - accuracy: 0.9571 - val_loss: 0.4047 - val_accuracy: 0.8894\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.1463 - accuracy: 0.9581 - val_loss: 0.4222 - val_accuracy: 0.8842\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 0.2705 - accuracy: 0.8958\n",
      "Test acc: 0.896\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "validation_data=binary_2gram_val_ds.cache(),\n",
    "epochs=10,\n",
    "callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888ae23-0dbc-4001-9b54-a5410426c4fa",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "194f9e89-d2b2-4ff5-ab56-3dd06eafa5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.95 percent positive\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "[\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69009877-8c29-40f5-ae6a-437f89452cb4",
   "metadata": {},
   "source": [
    "# Sequence model: using word embeddings instead of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "113aba83-dc09-4cda-9c6e-d91a773b918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "max_tokens=max_tokens,\n",
    "output_mode=\"int\",\n",
    "output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(\n",
    "lambda x, y: (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "lambda x, y: (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "lambda x, y: (text_vectorization(x), y),\n",
    "num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5b872e1-c84a-4b3e-a642-7b39b7e9428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 64)                73984     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5194049 (19.81 MB)\n",
      "Trainable params: 5194049 (19.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "loss=\"binary_crossentropy\",\n",
    "metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d0e99e5-9174-40c1-996d-b4e83075560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 15:56:03.425827: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond_36/output/_23'\n",
      "2024-02-23 15:56:04.570002: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
      "2024-02-23 15:56:05.702664: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f68ec774dc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-23 15:56:05.702706: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n",
      "2024-02-23 15:56:05.727218: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708703765.819143     330 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 72s 103ms/step - loss: 0.4549 - accuracy: 0.7834 - val_loss: 0.3319 - val_accuracy: 0.8580\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 50s 81ms/step - loss: 0.2891 - accuracy: 0.8852 - val_loss: 0.3305 - val_accuracy: 0.8696\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 48s 76ms/step - loss: 0.2157 - accuracy: 0.9196 - val_loss: 0.3026 - val_accuracy: 0.8718\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.1650 - accuracy: 0.9407 - val_loss: 0.3976 - val_accuracy: 0.8626\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.1242 - accuracy: 0.9557 - val_loss: 0.3674 - val_accuracy: 0.8756\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.0932 - accuracy: 0.9667 - val_loss: 0.3843 - val_accuracy: 0.8672\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.0724 - accuracy: 0.9763 - val_loss: 0.5497 - val_accuracy: 0.8638\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 46s 74ms/step - loss: 0.0573 - accuracy: 0.9812 - val_loss: 0.6574 - val_accuracy: 0.8398\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 47s 76ms/step - loss: 0.0479 - accuracy: 0.9841 - val_loss: 0.7947 - val_accuracy: 0.8280\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.0372 - accuracy: 0.9876 - val_loss: 0.9853 - val_accuracy: 0.8306\n",
      "782/782 [==============================] - 37s 43ms/step - loss: 0.3064 - accuracy: 0.8724\n",
      "Test acc: 0.872\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9ee2a-a5e5-4517-9d4c-b73d1a545806",
   "metadata": {},
   "source": [
    "# Transformer Encoder with position aware Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ead09924-0332-471f-8bc3-678696cbe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization(epsilon=1e-7)\n",
    "        self.layernorm_2 = layers.LayerNormalization(epsilon=1e-7)\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"embed_dim\": self.embed_dim,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51b4de70-acb7-4cdf-9382-69b69f328d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "        \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9c926-eec2-4e5c-8c97-b874cbc48e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posi  (None, None, 256)         5273600   \n",
      " tionalEmbedding)                                                \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, None, 256)         543776    \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 256)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5817633 (22.19 MB)\n",
      "Trainable params: 5817633 (22.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "625/625 [==============================] - 48s 72ms/step - loss: 0.5311 - accuracy: 0.7372 - val_loss: 0.4411 - val_accuracy: 0.7994\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.2994 - accuracy: 0.8719 - val_loss: 0.3254 - val_accuracy: 0.8624\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.2322 - accuracy: 0.9092 - val_loss: 0.3109 - val_accuracy: 0.8700\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.1921 - accuracy: 0.9244 - val_loss: 0.2970 - val_accuracy: 0.8824\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.1632 - accuracy: 0.9385 - val_loss: 0.3196 - val_accuracy: 0.8810\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.1382 - accuracy: 0.9485 - val_loss: 0.3471 - val_accuracy: 0.8784\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.1149 - accuracy: 0.9577 - val_loss: 0.4119 - val_accuracy: 0.8756\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 32s 50ms/step - loss: 0.0924 - accuracy: 0.9661 - val_loss: 0.4888 - val_accuracy: 0.8604\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 32s 50ms/step - loss: 0.0737 - accuracy: 0.9740 - val_loss: 0.6193 - val_accuracy: 0.8564\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 32s 50ms/step - loss: 0.0629 - accuracy: 0.9780 - val_loss: 0.4894 - val_accuracy: 0.8742\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0453 - accuracy: 0.9841 - val_loss: 0.6847 - val_accuracy: 0.8702\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 32s 50ms/step - loss: 0.0358 - accuracy: 0.9877 - val_loss: 0.5413 - val_accuracy: 0.8668\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0277 - accuracy: 0.9903 - val_loss: 0.8277 - val_accuracy: 0.8656\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 32s 50ms/step - loss: 0.0255 - accuracy: 0.9917 - val_loss: 0.8080 - val_accuracy: 0.8654\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.0219 - accuracy: 0.9927 - val_loss: 0.9647 - val_accuracy: 0.8680\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.0169 - accuracy: 0.9940 - val_loss: 1.0287 - val_accuracy: 0.8690\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.0152 - accuracy: 0.9947 - val_loss: 1.1855 - val_accuracy: 0.8522\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.0147 - accuracy: 0.9946 - val_loss: 1.1079 - val_accuracy: 0.8676\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 1.1078 - val_accuracy: 0.8700\n",
      "Epoch 20/20\n",
      "384/625 [=================>............] - ETA: 10s - loss: 0.0111 - accuracy: 0.9967"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "loss=\"binary_crossentropy\",\n",
    "metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20,\n",
    "callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "\"full_transformer_encoder.keras\",\n",
    "custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "\"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa773de3-3515-4dfd-839d-41d1166b5681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
